{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c645630",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#SPD-and-HFM\" data-toc-modified-id=\"SPD-and-HFM-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>SPD and HFM</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c777512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# 1. IMPORTS\n",
    "# ======================================================================\n",
    "import os\n",
    "import copy\n",
    "import shutil\n",
    "import sys\n",
    "import glob\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Geostatistics, HFM, and other packages\n",
    "import pygeostat as gs\n",
    "from agd import Eikonal\n",
    "from agd.Metrics import Riemann\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy import spatial\n",
    "from scipy.signal import remez, minimum_phase, freqz, group_delay\n",
    "from scipy import io\n",
    "from decimal import Decimal\n",
    "from statsmodels.graphics import tsaplots\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Neural network packages\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# For hyperparameter tuning\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "from hyperopt.pyll import scope\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07718d02",
   "metadata": {},
   "source": [
    "## SPD and HFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b34c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# 2. DIRECTORIES AND DATA LOADING\n",
    "# ======================================================================\n",
    "outdir = 'Output/'\n",
    "exedir = 'exe/'\n",
    "\n",
    "# Load the grid definition from a file (adjust filename as needed)\n",
    "griddef = gs.GridDef(grid_file='griddef.txt')\n",
    "print(griddef)\n",
    "\n",
    "# Load borehole (or sample) data (adjust filename/columns as needed)\n",
    "bhdata = gs.DataFile('cudata.dat', griddef=griddef, readfl=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1119b5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "LVA =gs.DataFile('./Output/LVA.out', griddef = griddef)\n",
    "LVA.addcoord()\n",
    "LVA.data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6b20da",
   "metadata": {},
   "outputs": [],
   "source": [
    "LVA.data = LVA.data.rename(columns={'angle1': 'strike','angle2': 'dip','angle3': 'plunge', \n",
    "                                    'ranges12': 'r1',  'ranges13' :'r2'} )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a997160",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.write_gslib(LVA.data, './Output/LVA2.out')\n",
    "LVA =gs.DataFile('./Output/LVA2.out', griddef = griddef)\n",
    "LVA.data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e436ed83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, ensure theyâ€™re not below 0.001\n",
    "minval = 1e-3\n",
    "LVA.data['r1'] = LVA.data['r1'].clip(lower=minval)\n",
    "LVA.data['r2'] = LVA.data['r2'].clip(lower=minval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b64e2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "LVA.data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd54577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_grid_points_rev3D(griddef, order='GSLIB'):\n",
    "    \"\"\"\n",
    "    Generate (y, z, x) coords for the FULL grid, \n",
    "    shaped (nz, ny, nx) if you want that ordering.\n",
    "    \n",
    "    We'll interpret:\n",
    "      axis0 => y,\n",
    "      axis1 => z,\n",
    "      axis2 => x,\n",
    "    consistent with dims=[nz,ny,nx] in HFM.\n",
    "    \"\"\"\n",
    "    ny, nz, nx = griddef.ny, griddef.nz, griddef.nx\n",
    "    \n",
    "    # y-coords\n",
    "    y_vals = np.linspace(griddef.ylimits[0],\n",
    "                         griddef.ylimits[1] - griddef.ysiz,\n",
    "                         ny)\n",
    "    # z-coords\n",
    "    z_vals = np.linspace(griddef.zlimits[0],\n",
    "                         griddef.zlimits[1] - griddef.zsiz,\n",
    "                         nz)\n",
    "    # x-coords\n",
    "    x_vals = np.linspace(griddef.xlimits[0],\n",
    "                         griddef.xlimits[1] - griddef.xsiz,\n",
    "                         nx)\n",
    "    \n",
    "    # indexing='ij' => G shape (ny, nz, nx) if we do Y, Z, X\n",
    "    # but we actually want shape (nz, ny, nx), so let's reorder:\n",
    "    Y, Z, X = np.meshgrid(y_vals, z_vals, x_vals, indexing='ij')\n",
    "    # Y.shape => (ny, nz, nx)\n",
    "    # We'll interpret axis0 => y, axis1 => z, axis2 => x\n",
    "    \n",
    "    if order=='GSLIB':\n",
    "        coords = np.column_stack([Y.ravel(order='C'),\n",
    "                                  Z.ravel(order='C'),\n",
    "                                  X.ravel(order='C')])\n",
    "    else:\n",
    "        coords = np.column_stack([Y.ravel(order='F'),\n",
    "                                  Z.ravel(order='F'),\n",
    "                                  X.ravel(order='F')])\n",
    "    return coords\n",
    "\n",
    "\n",
    "def landmark_points_rev3D(Ly, Lz, Lx, griddef, plot=False):\n",
    "    \"\"\"\n",
    "    3D extension for landmark points, specifically in (y,z,x) order.\n",
    "    \n",
    "    Ly, Lz, Lx = number of landmark seeds along y,z,x.\n",
    "    We'll interpret dims=[nz, ny, nx].\n",
    "    \"\"\"\n",
    "    ny, nz, nx = griddef.ny, griddef.nz, griddef.nx\n",
    "    \n",
    "    # intervals\n",
    "    interval_y = ny/(Ly + 1)\n",
    "    interval_z = nz/(Lz + 1)\n",
    "    interval_x = nx/(Lx + 1)\n",
    "    \n",
    "    # actual domain range\n",
    "    y_vals = np.linspace(griddef.ylimits[0] + interval_y*griddef.ysiz,\n",
    "                         griddef.ylimits[1] - interval_y*griddef.ysiz,\n",
    "                         Ly)\n",
    "    z_vals = np.linspace(griddef.zlimits[0] + interval_z*griddef.zsiz,\n",
    "                         griddef.zlimits[1] - interval_z*griddef.zsiz,\n",
    "                         Lz)\n",
    "    x_vals = np.linspace(griddef.xlimits[0] + interval_x*griddef.xsiz,\n",
    "                         griddef.xlimits[1] - interval_x*griddef.xsiz,\n",
    "                         Lx)\n",
    "    \n",
    "    # mesh in (y,z,x) => shape (Ly, Lz, Lx)\n",
    "    Yv, Zv, Xv = np.meshgrid(y_vals, z_vals, x_vals, indexing='ij')\n",
    "    \n",
    "    if plot:\n",
    "        import matplotlib.pyplot as plt\n",
    "        from mpl_toolkits.mplot3d import Axes3D\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.scatter(Yv, Zv, Xv, c='k')\n",
    "        ax.set_title(f\"Landmark Points: (Ly={Ly}, Lz={Lz}, Lx={Lx})\")\n",
    "        plt.show()\n",
    "    \n",
    "    return Yv, Zv, Xv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca84ee50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def angles_to_unit_vector_3D(strike_deg, dip_deg, plunge_deg):\n",
    "    \"\"\"\n",
    "    Convert (strike, dip, plunge) in degrees -> (uX, uY, uZ).\n",
    "    But be mindful we interpret the final 3D as (y,z,x)...\n",
    "\n",
    "    Suppose you keep the same formula:\n",
    "      ux = sin(strike)*cos(dip)\n",
    "      uy = cos(strike)*cos(dip)\n",
    "      uz = -sin(dip)\n",
    "    \n",
    "    We'll rename them as uX, uY, uZ for clarity, but watch your usage.\n",
    "    \"\"\"\n",
    "    s = np.radians(strike_deg)\n",
    "    d = np.radians(dip_deg)\n",
    "    # ignoring plunge for now\n",
    "    \n",
    "    uX = np.sin(s)*np.cos(d)\n",
    "    uY = np.cos(s)*np.cos(d)\n",
    "    uZ = -np.sin(d)\n",
    "    \n",
    "    length = np.sqrt(uX**2 + uY**2 + uZ**2)\n",
    "    if length>1e-12:\n",
    "        uX, uY, uZ = uX/length, uY/length, uZ/length\n",
    "    return uX, uY, uZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70afe425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SPD_HFM_3D_rev10(hfmIn, LVA, griddef, N_data,\n",
    "                     order='GSLIB', geodesics=False, tips=(4,4,4),\n",
    "                     L_points=(10,10,10)):\n",
    "    \"\"\"\n",
    "    3D version of your SPD_HFM_rev10, using (y,z,x) axis order.\n",
    "    \n",
    "    We interpret:\n",
    "      dims=[nz, ny, nx] in hfmIn.SetRect\n",
    "      arrays => shape (nz, ny, nx, order='C')\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    ny, nz, nx = griddef.ny, griddef.nz, griddef.nx\n",
    "    \n",
    "    # 1) Extract columns\n",
    "    if 'r1' not in LVA.data.columns or 'r2' not in LVA.data.columns:\n",
    "        raise ValueError(\"LVA must have r1, r2 columns!\")\n",
    "    if 'strike' not in LVA.data.columns or 'dip' not in LVA.data.columns:\n",
    "        raise ValueError(\"Need strike, dip columns!\")\n",
    "    \n",
    "    # Clipping to avoid zero\n",
    "    LVA.data['r1'] = LVA.data['r1'].clip(lower=1e-3)\n",
    "    LVA.data['r2'] = LVA.data['r2'].clip(lower=1e-3)\n",
    "    \n",
    "    strike_arr = LVA['strike'].values\n",
    "    dip_arr    = LVA['dip'].values\n",
    "    r1_arr     = LVA['r1'].values\n",
    "    r2_arr     = LVA['r2'].values\n",
    "    \n",
    "    if len(strike_arr)!=N_data:\n",
    "        raise ValueError(\"Mismatch in LVA size vs N_data!\")\n",
    "    \n",
    "    # 2) Reshape to (nz, ny, nx) in row-major => 'C'\n",
    "    # But note typical usage => the first index is z, second is y, third is x\n",
    "    # => shape (nz, ny, nx)\n",
    "    strike_3d = strike_arr.reshape(nz, ny, nx, order='C')\n",
    "    dip_3d    = dip_arr.reshape(nz, ny, nx, order='C')\n",
    "    r1_3d     = r1_arr.reshape(nz, ny, nx, order='C')\n",
    "    r2_3d     = r2_arr.reshape(nz, ny, nx, order='C')\n",
    "    \n",
    "    # Build orientation arrays\n",
    "    uX_3d = np.zeros((nz, ny, nx), dtype=float)\n",
    "    uY_3d = np.zeros((nz, ny, nx), dtype=float)\n",
    "    uZ_3d = np.zeros((nz, ny, nx), dtype=float)\n",
    "    \n",
    "    for k in range(nz):\n",
    "        for j in range(ny):\n",
    "            for i in range(nx):\n",
    "                st_deg = strike_3d[k,j,i]\n",
    "                dp_deg = dip_3d[k,j,i]\n",
    "                ux, uy, uz = angles_to_unit_vector_3D(st_deg, dp_deg, 0.0)\n",
    "                uX_3d[k,j,i] = ux\n",
    "                uY_3d[k,j,i] = uy\n",
    "                uZ_3d[k,j,i] = uz\n",
    "    \n",
    "    alpha = 1.0 / r1_3d\n",
    "    beta  = 1.0 / r2_3d\n",
    "    \n",
    "    # 3) Build metric => Riemann.needle((uX_3d,uY_3d,uZ_3d), alpha,beta).dual()\n",
    "    hfmIn['metric'] = Riemann.needle(\n",
    "        (uX_3d, uY_3d, uZ_3d),\n",
    "        alpha,\n",
    "        beta\n",
    "    ).dual()\n",
    "    \n",
    "    if geodesics:\n",
    "        hfmIn.SetUniformTips(tips)\n",
    "    hfmIn['exportValues'] = 1\n",
    "    hfmIn['exportGeodesicFlow'] = 0\n",
    "    \n",
    "    path_vis = []\n",
    "    \n",
    "    # 4) SPD: Full vs. Landmark approach\n",
    "    if L_points==0:\n",
    "        # Full NxNyNz => NxN SPD\n",
    "        coords_3d = generate_grid_points_rev3D(griddef, order=order)  # shape => (N_data,3)\n",
    "        # coords_3d[:,0] => y, coords_3d[:,1]=> z, coords_3d[:,2]=> x\n",
    "        N_h_spd = N_data\n",
    "        # NxN SPD => watch memory if large\n",
    "        h_spd_mtx = np.zeros((N_data, N_data), dtype=np.float32)\n",
    "        \n",
    "        # partial vector\n",
    "        N_pairs = np.sum(range(N_data))\n",
    "        h_spd_vecc = np.zeros((N_pairs,1), dtype=np.float32)\n",
    "        i = 0\n",
    "        for i1 in tqdm(range(N_h_spd-1)):\n",
    "            sy, sz, sx = coords_3d[i1,:]\n",
    "            hfmIn['seed'] = [sy, sz, sx]  # (y,z,x)\n",
    "            hfmOut = hfmIn.Run()\n",
    "            \n",
    "            if geodesics:\n",
    "                path_vis.append(hfmOut['geodesics'])\n",
    "            \n",
    "            # flatten\n",
    "            if order=='GSLIB':\n",
    "                dist_flat = hfmOut['values'].flatten(order='C')\n",
    "            else:\n",
    "                dist_flat = hfmOut['values'].flatten(order='F')\n",
    "            \n",
    "            i2_len = N_data - (i1+1)\n",
    "            h_spd_vecc[i:i+i2_len,0] = dist_flat[i1+1:]\n",
    "            h_spd_mtx[i1+1:, i1] = h_spd_vecc[i:i+i2_len,0]\n",
    "            i += i2_len\n",
    "        \n",
    "        # Mirror\n",
    "        h_spd_mtx = h_spd_mtx + h_spd_mtx.T\n",
    "    \n",
    "    else:\n",
    "        # Landmark approach => shape [N_landmarks, N_data]\n",
    "        Ly, Lz, Lx = L_points\n",
    "        Yv, Zv, Xv = landmark_points_rev3D(Ly, Lz, Lx, griddef, plot=False)\n",
    "        \n",
    "        if order=='GSLIB':\n",
    "            source_y = Yv.flatten(order='C')\n",
    "            source_z = Zv.flatten(order='C')\n",
    "            source_x = Xv.flatten(order='C')\n",
    "        else:\n",
    "            source_y = Yv.flatten(order='F')\n",
    "            source_z = Zv.flatten(order='F')\n",
    "            source_x = Xv.flatten(order='F')\n",
    "        \n",
    "        N_h_spd = Ly*Lz*Lx\n",
    "        h_spd_mtx = np.zeros((N_h_spd, N_data), dtype=np.float32)\n",
    "        \n",
    "        for i1 in tqdm(range(N_h_spd)):\n",
    "            sy, sz, sx = source_y[i1], source_z[i1], source_x[i1]\n",
    "            hfmIn['seed'] = [sy, sz, sx]\n",
    "            hfmOut = hfmIn.Run()\n",
    "            \n",
    "            if geodesics:\n",
    "                path_vis.append(hfmOut['geodesics'])\n",
    "            \n",
    "            if order=='GSLIB':\n",
    "                dist_flat = hfmOut['values'].flatten(order='C')\n",
    "            else:\n",
    "                dist_flat = hfmOut['values'].flatten(order='F')\n",
    "            \n",
    "            h_spd_mtx[i1,:] = dist_flat\n",
    "    \n",
    "    return h_spd_mtx, path_vis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad22452c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import pygeostat as gs\n",
    "\n",
    "    hfmIn = Eikonal.dictIn({\n",
    "        'arrayOrdering': 'RowMajor',  # similar to 2D\n",
    "        'model': 'Riemann3',         # now 3D\n",
    "        'seedValue': 0,\n",
    "        'geodesicStep':0.1,\n",
    "        'order': 5,\n",
    "    })\n",
    "\n",
    "    # Example: sides=( (ymin,ymax),(zmin,zmax),(xmin,xmax) ), dims=[nz, ny, nx]\n",
    "    hfmIn.SetRect(\n",
    "        sides=[[griddef.ylimits[0], griddef.ylimits[1]],\n",
    "               [griddef.zlimits[0], griddef.zlimits[1]],\n",
    "               [griddef.xlimits[0], griddef.xlimits[1]]],\n",
    "        dims=[griddef.nz, griddef.ny, griddef.nx]\n",
    "    )\n",
    "    \n",
    "    # N_data = griddef.count()\n",
    "    # SPD call\n",
    "    h_spd_mtx, path_vis = SPD_HFM_3D_rev10(\n",
    "        hfmIn, LVA, griddef,\n",
    "        N_data=griddef.count(),\n",
    "        order='GSLIB',\n",
    "        geodesics=True,\n",
    "        tips=(4,4,4),\n",
    "        L_points=(5,5,5)  \n",
    "    )\n",
    "    print(\"SPD shape:\", h_spd_mtx.shape)\n",
    "    if path_vis:\n",
    "        print(\"Geodesics found:\", len(path_vis))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a44a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_spd = pd.DataFrame(h_spd_mtx.T)\n",
    "# gs.write_gslib(df_spd, 'spd3d.out')   \n",
    "# data  = gs.DataFile('spd3d.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9d687e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Select 9 variables from 0 to 120 in increments of 15\n",
    "# variables = [str(i) for i in range(0, 8, 1)]  # ['0', '15', '30', '45', '60', '75', '90', '105', '120']\n",
    "\n",
    "# # Define the slice index (e.g., z=10)\n",
    "# slice_number = 15\n",
    "\n",
    "# # Create a 3x3 grid of subplots\n",
    "# fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(12, 12))\n",
    "\n",
    "# # Flatten the 2D array of axes to iterate easily\n",
    "# axes_flat = axes.ravel()\n",
    "\n",
    "# # Loop through each variable and axis\n",
    "# for var, ax in zip(variables, axes_flat):\n",
    "#     gs.slice_plot(\n",
    "#         data,\n",
    "#         var=var,\n",
    "#         griddef=griddef,\n",
    "#         ax=ax,\n",
    "#         orient='xy',\n",
    "#         slice_number=slice_number,\n",
    "#         cbar=False,\n",
    "#         title=f\"Distance from seed[0]\\nz={slice_number}, Var={var}\"\n",
    "#     )\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566bf743",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,z = griddef.get_coordinates()\n",
    "grid_coords= np.hstack((x.reshape(len(x),1),y.reshape((len(y)),1),z.reshape((len(z)),1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5519cbd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Hyperopt\n",
    "from hyperopt import fmin, tpe, hp, Trials\n",
    "\n",
    "# ==================================================================================\n",
    "# 1. SPD Computation + Standardization\n",
    "# ==================================================================================\n",
    "mean_value = np.mean(h_spd_mtx)\n",
    "std_value = np.std(h_spd_mtx)\n",
    "\n",
    "# Scale SPD matrix\n",
    "h_spd_HFM_scaled = (h_spd_mtx - mean_value) / (std_value + 1e-12)\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Create a DataFrame for grid coordinates\n",
    "# ----------------------------------------------------------------------------------\n",
    "df1 = pd.DataFrame(grid_coords, columns=['X', 'Y', 'Z'])\n",
    "\n",
    "# Optionally normalize coordinates if you want them as features\n",
    "lon = df1['X'].values\n",
    "lat = df1['Y'].values\n",
    "elev = df1['Z'].values\n",
    "\n",
    "normalized_lon = (lon - lon.min()) / (lon.max() - lon.min())\n",
    "normalized_lat = (lat - lat.min()) / (lat.max() - lat.min())\n",
    "normalized_elev = (elev - elev.min()) / (elev.max() - elev.min())\n",
    "\n",
    "s_test = np.vstack((normalized_lon, normalized_lat, normalized_elev)).T\n",
    "\n",
    "# ==================================================================================\n",
    "# Outer 5-Fold Loop (Nested CV: Outer Loop)\n",
    "# ==================================================================================\n",
    "for fold in range(1, 6):\n",
    "    print(f\"\\n========================\")\n",
    "    print(f\" OUTER FOLD {fold}/5\")\n",
    "    print(f\"========================\\n\")\n",
    "\n",
    "    # ------------------------------------------------------------------------------\n",
    "    # 2. Load outer-train data\n",
    "    # ------------------------------------------------------------------------------\n",
    "    Traindat = gs.DataFile(f'Training0{fold}.dat', x='X', y='Y', z='Z')\n",
    "    df_train = Traindat.data\n",
    "\n",
    "    # Map training points to SPD matrix rows\n",
    "    near_points = df1[['X', 'Y', 'Z']].values\n",
    "    tree = spatial.KDTree(near_points)\n",
    "    idx = tree.query(df_train[['X', 'Y', 'Z']].values)[1]\n",
    "\n",
    "    # Group by neighbor index and compute mean of 'Cu'\n",
    "    df_train_neighbor = df_train.assign(neighbor=idx)\n",
    "    df_cu = df_train_neighbor.groupby('neighbor')['Cu'].mean()\n",
    "\n",
    "    idx_new = df_cu.index.values\n",
    "    pm25 = df_cu.values  # Target variable\n",
    "\n",
    "    # Build feature matrix\n",
    "    phi_combined = h_spd_HFM_scaled.T  # shape => (N_data, #landmarks)\n",
    "    X_full = phi_combined[idx_new, :]\n",
    "    y_full = pm25.reshape(-1, 1)\n",
    "\n",
    "    # ------------------------------------------------------------------------------\n",
    "    # 3. Define model-creation function\n",
    "    # ------------------------------------------------------------------------------\n",
    "    def create_model(hparams, input_dim):\n",
    "        \"\"\"\n",
    "        Build a Keras model using hyperparameters.\n",
    "        Three hidden layers, each with Dropout and l1_l2 regularization.\n",
    "        \"\"\"\n",
    "        model = Sequential()\n",
    "\n",
    "        # Hidden layer 1\n",
    "        model.add(\n",
    "            Dense(\n",
    "                int(hparams['units']),\n",
    "                input_dim=input_dim,\n",
    "                kernel_initializer='he_uniform',\n",
    "                activation='relu',\n",
    "                kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-6, l2=1e-4)\n",
    "            )\n",
    "        )\n",
    "        model.add(Dropout(hparams['dropout']))\n",
    "\n",
    "        # Hidden layer 2\n",
    "        model.add(\n",
    "            Dense(\n",
    "                int(hparams['units']),\n",
    "                activation='relu',\n",
    "                kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-6, l2=1e-4)\n",
    "            )\n",
    "        )\n",
    "        model.add(Dropout(hparams['dropout']))\n",
    "\n",
    "        # Hidden layer 3\n",
    "        model.add(\n",
    "            Dense(\n",
    "                int(hparams['units']),\n",
    "                activation='relu',\n",
    "                kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-6, l2=1e-4)\n",
    "            )\n",
    "        )\n",
    "        model.add(Dropout(hparams['dropout']))\n",
    "\n",
    "        # Output layer\n",
    "        model.add(Dense(1, activation='linear'))\n",
    "\n",
    "        optimizer = Adam(learning_rate=hparams['learning_rate'])\n",
    "        model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "        return model\n",
    "\n",
    "    # ------------------------------------------------------------------------------\n",
    "    # 4. Inner 5-Fold CV for hyperparameter tuning (via Hyperopt)\n",
    "    # ------------------------------------------------------------------------------\n",
    "    def hyperopt_objective(hparams):\n",
    "        \"\"\"\n",
    "        For each set of hyperparams, perform 5-fold CV on (X_full, y_full).\n",
    "        Return the average MSE across the 5 folds as the objective to minimize.\n",
    "        \"\"\"\n",
    "        epochs = int(hparams['epochs'])\n",
    "        batch_size = int(hparams['batch_size'])\n",
    "\n",
    "        # Change the random seed here for each outer fold\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42 + fold)\n",
    "        fold_mses = []\n",
    "\n",
    "        for train_index, val_index in kf.split(X_full):\n",
    "            X_tr, X_val = X_full[train_index], X_full[val_index]\n",
    "            y_tr, y_val = y_full[train_index], y_full[val_index]\n",
    "\n",
    "            model = create_model(hparams, input_dim=X_full.shape[1])\n",
    "            model.fit(X_tr, y_tr, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "            y_val_pred = model.predict(X_val)\n",
    "            fold_mses.append(mean_squared_error(y_val, y_val_pred))\n",
    "\n",
    "        return np.mean(fold_mses)\n",
    "\n",
    "    # Define Hyperparameter search space\n",
    "    search_space = {\n",
    "        'units': hp.quniform('units', 400, 1000, 200),\n",
    "        'dropout': hp.uniform('dropout', 0.2, 0.5),\n",
    "        'learning_rate': hp.loguniform('learning_rate', np.log(1e-5), np.log(1e-2)),\n",
    "        'epochs': hp.quniform('epochs', 200, 800, 200),\n",
    "        'batch_size': hp.quniform('batch_size', 16, 64, 16),\n",
    "    }\n",
    "\n",
    "    # Run hyperparameter optimization with a different seed each outer fold\n",
    "    trials = Trials()\n",
    "    best = fmin(\n",
    "        fn=hyperopt_objective,\n",
    "        space=search_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=5,\n",
    "        trials=trials,\n",
    "        # Use a fold-dependent seed, e.g., 42+fold\n",
    "        rstate=np.random.default_rng(42 + fold)\n",
    "    )\n",
    "\n",
    "    # Convert selected params to correct data types\n",
    "    best['units'] = int(best['units'])\n",
    "    best['epochs'] = int(best['epochs'])\n",
    "    best['batch_size'] = int(best['batch_size'])\n",
    "\n",
    "    print(\"Best hyperparameters from inner CV:\")\n",
    "    print(best)\n",
    "\n",
    "    # ------------------------------------------------------------------------------\n",
    "    # 5. Retrain final model on the FULL outer-train data\n",
    "    # ------------------------------------------------------------------------------\n",
    "    def create_regularized_model_final(best_params, input_dim):\n",
    "        \"\"\"\n",
    "        Build a final model using the best hyperparams\n",
    "        from the inner CV, with l1/l2 fixed.\n",
    "        \"\"\"\n",
    "        model = Sequential()\n",
    "\n",
    "        # 3 hidden layers\n",
    "        for _ in range(3):\n",
    "            model.add(Dense(\n",
    "                best_params['units'],\n",
    "                input_dim=input_dim,\n",
    "                kernel_initializer='he_uniform',\n",
    "                activation='relu',\n",
    "                kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-6, l2=1e-4)\n",
    "            ))\n",
    "            model.add(Dropout(best_params['dropout']))\n",
    "            # Only specify input_dim on the first layer\n",
    "            input_dim = None\n",
    "\n",
    "        # Output layer\n",
    "        model.add(Dense(1, activation='linear'))\n",
    "\n",
    "        optimizer = Adam(learning_rate=best_params['learning_rate'])\n",
    "        model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "        return model\n",
    "\n",
    "    # Build and train the final model on the entire outer-train data\n",
    "    final_model = create_regularized_model_final(best, input_dim=X_full.shape[1])\n",
    "    final_model.fit(\n",
    "        X_full,\n",
    "        y_full,\n",
    "        epochs=best['epochs'],\n",
    "        batch_size=best['batch_size'],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------------------------\n",
    "    # 6. Predict on the entire domain\n",
    "    # ------------------------------------------------------------------------------\n",
    "    X_RBF_pred = phi_combined  # entire domain\n",
    "    PM25_pred = final_model.predict(X_RBF_pred)\n",
    "\n",
    "    # Clip negative predictions (optional)\n",
    "    PM25_pred[PM25_pred < 0] = 0\n",
    "\n",
    "    outfile = f'DeepKriging_{fold}.out'\n",
    "    gs.write_gslib(pd.DataFrame(PM25_pred), outfile)\n",
    "    print(f\"Domain predictions saved: {outfile}\")\n",
    "\n",
    "    # ------------------------------------------------------------------------------\n",
    "    # 7. Evaluate on validation set\n",
    "    # ------------------------------------------------------------------------------\n",
    "    testfile = gs.DataFile(flname=f'Validation0{fold}.dat', griddef=griddef, readfl=True)\n",
    "    DeepKriging = gs.DataFile(f'DeepKriging_{fold}.out', griddef=griddef)\n",
    "\n",
    "    idx, ingrid = griddef.get_index(\n",
    "        x=testfile.data['X'],\n",
    "        y=testfile.data['Y'],\n",
    "        z=testfile.data['Z']\n",
    "    )\n",
    "\n",
    "    testfile.data = testfile.data[idx >= 0]\n",
    "    idx = idx[idx >= 0]\n",
    "\n",
    "    # Print R^2 score on the test set\n",
    "    print(\n",
    "        r2_score(\n",
    "            testfile['Cu'],\n",
    "            DeepKriging.data['0'][idx].values\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb156dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# 9. COND. DEEP KRIGING \n",
    "# ======================================================================\n",
    "for fold in range(1,6):\n",
    "    deepkriging = gs.DataFile(f'DeepKriging_{fold}.out', griddef=griddef)\n",
    "    moment = gs.DataFile(f'./IDW/Debug_{fold}.out', x='APX', y='APY')\n",
    "\n",
    "    # Compute scaled variance for weighting\n",
    "    moment.data['Variance'] = moment.data['Local std.dev.']**2\n",
    "    moment.data['Scaled_Variance'] = (\n",
    "        moment.data['Variance'] - moment.data['Variance'].min()\n",
    "    ) / (\n",
    "        moment.data['Variance'].max() - moment.data['Variance'].min()\n",
    "    )\n",
    "\n",
    "    weight = moment['Scaled_Variance'] ** 0.25\n",
    "    cond_DK = deepkriging['0'] * np.array(weight) + moment['Local Mean'] * (1 - np.array(weight))\n",
    "\n",
    "    # Save conditional DK\n",
    "    gs.write_gslib(pd.DataFrame(cond_DK), f'./IDW/Weighted_DK_{fold}.out')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d005705",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "\n",
    "OK_R2 = 0\n",
    "DeepKriging_R2 = 0\n",
    "Cond_DK_R2 = 0\n",
    "OK_Ensemble = 0\n",
    "DeepKriging_Ensemble = 0\n",
    "Cond_DK_Ensemble = 0\n",
    "\n",
    "for fold in range(1, 6):\n",
    "    # Read the validation file\n",
    "    testfile = gs.DataFile(flname='Validation0{}.dat'.format(fold),\n",
    "                           griddef=griddef, readfl=True)\n",
    "\n",
    "    # Read the kriging outputs\n",
    "    OK = gs.DataFile('OK_{}.out'.format(fold), griddef=griddef)\n",
    "    # Ensure no negative estimates\n",
    "    OK.data.loc[OK.data['Estimate'] < 0, 'Estimate'] = 0\n",
    "\n",
    "    DeepKriging = gs.DataFile('DeepKriging_{}.out'.format(fold), griddef=griddef)\n",
    "    Cond_DK = gs.DataFile('./IDW/Weighted_DK_{}.out'.format(fold), griddef=griddef)\n",
    "\n",
    "    # Get indices for testfile data points\n",
    "    idx, ingrid = griddef.get_index(x=testfile.data['X'],\n",
    "                                    y=testfile.data['Y'],\n",
    "                                    z=testfile.data['Z'])\n",
    "    testfile.data = testfile.data[idx >= 0]\n",
    "    idx = idx[idx >= 0]\n",
    "\n",
    "    # Create subplots\n",
    "    f, axes = plt.subplots(1, 3, figsize=(12, 3.5))\n",
    "\n",
    "    # OK plot\n",
    "    gs.validation_plot(\n",
    "        testfile['Cu'], OK.data['Estimate'][idx].values,\n",
    "        grid=True, stat_blk='minimal', ms=2,\n",
    "        title='OK Fold {}'.format(fold), figsize=(4, 4),\n",
    "        ax=axes[0], vlim=(0, 1.5)\n",
    "    )\n",
    "    axes[0].text(\n",
    "        0.19, 1.3,\n",
    "        'R2: {}'.format(round(r2_score(testfile['Cu'], OK.data['Estimate'][idx].values), 2)),\n",
    "        horizontalalignment='center', verticalalignment='center',\n",
    "        fontsize=12, fontweight='bold'\n",
    "    )\n",
    "\n",
    "    # DeepKriging plot\n",
    "    gs.validation_plot(\n",
    "        testfile['Cu'], DeepKriging.data['0'][idx].values,\n",
    "        grid=True, stat_blk='minimal', ms=2,\n",
    "        title='DeepKriging_Fold {}'.format(fold), figsize=(4, 4),\n",
    "        ax=axes[1], vlim=(0, 1.5)\n",
    "    )\n",
    "    axes[1].text(\n",
    "        0.19, 1.3,\n",
    "        'R2: {}'.format(round(r2_score(testfile['Cu'], DeepKriging.data['0'][idx].values), 2)),\n",
    "        horizontalalignment='center', verticalalignment='center',\n",
    "        fontsize=12, fontweight='bold'\n",
    "    )\n",
    "\n",
    "    # Cond_DK plot\n",
    "    gs.validation_plot(\n",
    "        testfile['Cu'], Cond_DK.data['0'][idx].values,\n",
    "        grid=True, stat_blk='minimal', ms=2,\n",
    "        title='DeepKriging+IDW Fold {}'.format(fold), figsize=(4, 4),\n",
    "        ax=axes[2], vlim=(0, 1.5)\n",
    "    )\n",
    "    axes[2].text(\n",
    "        0.19, 1.3,\n",
    "        'R2: {}'.format(round(r2_score(testfile['Cu'], Cond_DK.data['0'][idx].values), 2)),\n",
    "        horizontalalignment='center', verticalalignment='center',\n",
    "        fontsize=12, fontweight='bold'\n",
    "    )\n",
    "\n",
    "    # Attempt to resolve aspect ratio warnings by resetting aspect\n",
    "    for a in axes:\n",
    "        a.set_aspect('auto')  # You may experiment with 'equal', 'datalim', etc.\n",
    "\n",
    "    # Update cumulative R2 scores\n",
    "    OK_R2 += r2_score(testfile['Cu'], OK.data['Estimate'][idx].values)\n",
    "    DeepKriging_R2 += r2_score(testfile['Cu'], DeepKriging.data['0'][idx].values)\n",
    "    Cond_DK_R2 += r2_score(testfile['Cu'], Cond_DK.data['0'][idx].values)\n",
    "\n",
    "    # Update ensemble predictions\n",
    "    OK_Ensemble += OK.data['Estimate'].values.reshape(-1, 1)\n",
    "    DeepKriging_Ensemble += DeepKriging.data['0'].values.reshape(-1, 1)\n",
    "    Cond_DK_Ensemble += Cond_DK.data['0'].values.reshape(-1, 1)\n",
    "\n",
    "# Compute average performances\n",
    "OK_Performance = round(OK_R2 / 5, 3)\n",
    "DeepKriging_Performance = round(DeepKriging_R2 / 5, 3)\n",
    "Cond_DK_Performance = round(Cond_DK_R2 / 5, 3)\n",
    "\n",
    "# Compute average ensemble predictions\n",
    "OK_Avg = OK_Ensemble / 5\n",
    "DeepKriging_Avg = DeepKriging_Ensemble / 5\n",
    "Cond_DK_Avg = Cond_DK_Ensemble / 5\n",
    "\n",
    "# Write outputs\n",
    "gs.write_gslib(pd.DataFrame(OK_Avg), 'OK_Average.out')\n",
    "gs.write_gslib(pd.DataFrame(DeepKriging_Avg), 'DeepKriging_Average.out')\n",
    "gs.write_gslib(pd.DataFrame(Cond_DK_Avg), 'Cond_DK_Average.out')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb97297",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(round(OK_Performance,2))\n",
    "print(round(DeepKriging_Performance,2))\n",
    "print(round(Cond_DK_Performance,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3752320d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Initialize RMSE values\n",
    "OK_RMSE = 0\n",
    "DeepKriging_RMSE = 0\n",
    "Cond_DK_RMSE = 0\n",
    "\n",
    "\n",
    "for fold in range(1, 6):\n",
    "    testfile = gs.DataFile(flname='Validation0{}.dat'.format(fold), griddef=griddef, readfl=True)\n",
    "    \n",
    "    OK = gs.DataFile('OK_{}.out'.format(fold), griddef=griddef)\n",
    "    OK.data['Estimate'][OK.data['Estimate'] < 0] = 0\n",
    "    \n",
    "    DeepKriging = gs.DataFile('DeepKriging_{}.out'.format(fold), griddef=griddef)   \n",
    "    Cond_DK = gs.DataFile('./IDW/Weighted_DK_{}.out'.format(fold), griddef=griddef)\n",
    "    \n",
    "    idx, ingrid = griddef.get_index(x=testfile.data['X'], y=testfile.data['Y'], z=testfile.data['Z'])\n",
    "    testfile.data = testfile.data[idx >= 0]\n",
    "    idx = idx[idx >= 0]    \n",
    "    \n",
    "    \n",
    "    OK_rmse = np.sqrt(mean_squared_error(testfile['Cu'], OK.data['Estimate'][idx].values))\n",
    "    DeepKriging_rmse = np.sqrt(mean_squared_error(testfile['Cu'], DeepKriging.data['0'][idx].values))\n",
    "    Cond_DK_rmse = np.sqrt(mean_squared_error(testfile['Cu'], Cond_DK.data['0'][idx].values))\n",
    "    \n",
    "    \n",
    "    OK_RMSE += OK_rmse\n",
    "    DeepKriging_RMSE += DeepKriging_rmse   \n",
    "    Cond_DK_RMSE += Cond_DK_rmse\n",
    "\n",
    "\n",
    "OK_Performance = round(OK_RMSE/5, 3)\n",
    "DeepKriging_Performance = round(DeepKriging_RMSE/5, 3)    \n",
    "Cond_DK_Performance = round(Cond_DK_RMSE/5, 3)  \n",
    "    \n",
    "\n",
    "\n",
    "print(round(OK_Performance,3))\n",
    "print(round(DeepKriging_Performance,3))\n",
    "print(round(Cond_DK_Performance,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2def03f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ok_results = gs.DataFile('OK_Average.out')\n",
    "deepkriging_results = gs.DataFile('DeepKriging_Average.out')\n",
    "Cond_DK_results = gs.DataFile('Cond_DK_Average.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa423e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "# Plot the first histogram\n",
    "gs.histogram_plot(\n",
    "    deepkriging_results,\n",
    "    var='0',\n",
    "    ax=ax,\n",
    "    color='blue',\n",
    "    alpha=0.5,  \n",
    "    stat_xy=(0.5,0.95), \n",
    "    label='Deep Kriging Results'\n",
    ")\n",
    "\n",
    "# Plot the second histogram\n",
    "gs.histogram_plot(\n",
    "    bhdata,\n",
    "    var='Cu',\n",
    "    ax=ax,\n",
    "    color='orange',\n",
    "    alpha=0.5,  \n",
    "    label='BH Data'\n",
    ")\n",
    "\n",
    "# Customize labels, legend, and tick sizes\n",
    "ax.set_xlabel(\"Variable Value\", fontsize=14)\n",
    "ax.set_ylabel(\"Frequency\", fontsize=14)\n",
    "ax.tick_params(labelsize=12)  #\n",
    "plt.legend(fontsize=12)       \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ee5484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "\n",
    "fig1 = plt.figure(figsize=(10, 4))\n",
    "# Adjust the width_ratios to have three elements\n",
    "gs2 = gridspec.GridSpec(2, 2, height_ratios=[1, 1])\n",
    "\n",
    "# Define the axes using the custom gridspec layout\n",
    "ax0 = fig1.add_subplot(gs2[:, 0])\n",
    "ax1 = fig1.add_subplot(gs2[0, 1])\n",
    "ax2 = fig1.add_subplot(gs2[1, 1])\n",
    "\n",
    "\n",
    "gs.slice_plot(Cond_DK_results, var = '0', pointdata = bhdata, pointvar = 'Cu', griddef=griddef, \n",
    "              pointkws={'edgecolors': 'k', 's':5}, title = 'Cond DK Estimation - Plan View', ax = ax0,\n",
    "              slice_number = 20, aspect = 1.5)\n",
    "\n",
    "gs.slice_plot(Cond_DK_results, var = '0', pointdata = bhdata, pointvar = 'Cu', griddef=griddef, \n",
    "              pointkws={'edgecolors': 'k', 's':5}, title = 'Cond DK Estimation - E-W Section View', ax = ax1,\n",
    "              slice_number = 60,  aspect = 0.8, orient = 'xz')\n",
    "\n",
    "gs.slice_plot(Cond_DK_results, var = '0', pointdata = bhdata, pointvar = 'Cu', griddef=griddef, \n",
    "              pointkws={'edgecolors': 'k', 's':5}, title = 'Cond DK Estimation - N-S Section View', ax = ax2,\n",
    "              slice_number = 40, aspect = 0.5, orient = 'yz')\n",
    "\n",
    "# Adjust the layout spacing\n",
    "plt.tight_layout(h_pad = 3, w_pad = 1.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30c8570",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# kt3dnpar = \"\"\"                 Parameters for KT3DN\n",
    "#                  ********************\n",
    "# START OF PARAMETERS:\n",
    "# {datafl}                         -file with data\n",
    "# 0  {xyzcol} {varcol}  0          -columns for DH,X,Y,Z,var,sec var\n",
    "# -998.0    1.0e21                 -trimming limits\n",
    "# 0                                -option: 0=grid, 1=cross, 2=jackknife\n",
    "# xvk.dat                          -file with jackknife data\n",
    "# 1   2   0    3    0              -columns for X,Y,Z,vr and sec var\n",
    "# kt3dn_dataspacing.out            -data spacing analysis output file (see note)\n",
    "# 0    15.0                        -number to search (0 for no dataspacing analysis, rec. 10 or 20) and composite length\n",
    "# 0    100   0                     -debugging level: 0,3,5,10; max data for GSKV;output total weight of each data?(0=no,1=yes)\n",
    "# kt3dn.dbg-nkt3dn.sum             -file for debugging output (see note)\n",
    "# {outfl}                          -file for kriged output (see GSB note)\n",
    "# {griddef}\n",
    "# 5    5      5                    -x,y and z block discretization\n",
    "# 4    48    12    1              -min, max data for kriging,upper max for ASO,ASO incr\n",
    "# 0      0                        -max per octant, max per drillhole (0-> not used)\n",
    "# 600.0  600.0  400.0              -maximum search radii\n",
    "# 00.0   0   0.0                  -angles for search ellipsoid\n",
    "# 1                                -0=SK,1=OK,2=LVM(resid),3=LVM((1-w)*m(u))),4=colo,5=exdrift,6=ICCK\n",
    "# 0.180 0.6  0.8                   -mean (if 0,4,5,6), corr. (if 4 or 6), var. reduction factor (if 4)\n",
    "# 0 0 0 0 0 0 0 0 0                -drift: x,y,z,xx,yy,zz,xy,xz,zy\n",
    "# 0                                -0, variable; 1, estimate trend\n",
    "# extdrift.out                     -gridded file with drift/mean\n",
    "# 4                                -column number in gridded file\n",
    "# key_out.out                      -gridded file with keyout (see note)\n",
    "# 0    1                           -column (0 if no keyout) and value to keep\n",
    "# -1    0                          -  nst, nugget effect\n",
    "# 1   2   0.0   0.0   0.0           - it,cc,ang1,ang2,ang3\n",
    "#      100    100    100            - a_hmax, a_hmin, a_vert\n",
    "# \"\"\"\n",
    "# kt3dn = gs.Program(program = 'exe/kt3dn')\n",
    "\n",
    "# griddef = gs.GridDef(grid_file='griddef.txt')\n",
    "# griddef\n",
    "\n",
    "# Traindat = gs.DataFile ('cudata.dat', griddef = griddef,\n",
    "#                        readfl=True)\n",
    "\n",
    "    \n",
    "# kt3dn.run(parstr=kt3dnpar.format(datafl = Traindat.flname,\n",
    "#                                             xyzcol = Traindat.gscol(Traindat.xyz),\n",
    "#                                             varcol = Traindat.gscol('Cu'),                              \n",
    "#                                             griddef = griddef,\n",
    "#                                             outfl = 'IDW.out'))          \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedca6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDW = gs.DataFile('IDW.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dcc183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib import gridspec\n",
    "\n",
    "# fig1 = plt.figure(figsize=(10, 4))\n",
    "# # Adjust the width_ratios to have three elements\n",
    "# gs2 = gridspec.GridSpec(2, 2, height_ratios=[1, 1])\n",
    "\n",
    "# # Define the axes using the custom gridspec layout\n",
    "# ax0 = fig1.add_subplot(gs2[:, 0])\n",
    "# ax1 = fig1.add_subplot(gs2[0, 1])\n",
    "# ax2 = fig1.add_subplot(gs2[1, 1])\n",
    "\n",
    "\n",
    "# gs.slice_plot(IDW, var = 'Estimate', pointdata = bhdata, pointvar = 'Cu', griddef=griddef, \n",
    "#               pointkws={'edgecolors': 'k', 's':5}, title = 'Cond DK Estimation - Plan View', ax = ax0,\n",
    "#               slice_number = 20, aspect = 1.5)\n",
    "\n",
    "# gs.slice_plot(IDW, var = 'Estimate', pointdata = bhdata, pointvar = 'Cu', griddef=griddef, \n",
    "#               pointkws={'edgecolors': 'k', 's':5}, title = 'Cond DK Estimation - E-W Section View', ax = ax1,\n",
    "#               slice_number = 60,  aspect = 0.8, orient = 'xz')\n",
    "\n",
    "# gs.slice_plot(IDW, var = 'Estimate', pointdata = bhdata, pointvar = 'Cu', griddef=griddef, \n",
    "#               pointkws={'edgecolors': 'k', 's':5}, title = 'Cond DK Estimation - N-S Section View', ax = ax2,\n",
    "#               slice_number = 40, aspect = 0.5, orient = 'yz')\n",
    "\n",
    "# # Adjust the layout spacing\n",
    "# plt.tight_layout(h_pad = 3, w_pad = 1.5)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d029c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import platform\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import pygeostat as gs\n",
    "import tqdm\n",
    "import scipy\n",
    "import statsmodels\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import hyperopt\n",
    "\n",
    "print(\"System platform:\", platform.platform())\n",
    "print(\"Python version :\", sys.version)\n",
    "print(\"NumPy version  :\", np.__version__)\n",
    "print(\"Pandas version :\", pd.__version__)\n",
    "print(\"Matplotlib version  :\", mpl.__version__)\n",
    "print(\"Seaborn version     :\", sns.__version__)\n",
    "print(\"pygeostat version   :\", getattr(gs, '__version__', 'No version attribute'))\n",
    "print(\"tqdm version        :\", tqdm.__version__)\n",
    "print(\"SciPy version       :\", scipy.__version__)\n",
    "print(\"statsmodels version :\", statsmodels.__version__)\n",
    "print(\"scikit-learn version:\", sklearn.__version__)\n",
    "print(\"TensorFlow version  :\", tf.__version__)\n",
    "print(\"hyperopt version    :\", hyperopt.__version__)\n",
    "\n",
    "# If AGD/Eikonal has a version attribute:\n",
    "# from agd import Eikonal\n",
    "# print(\"AGD/Eikonal version:\", getattr(Eikonal, '__version__', 'No version attribute'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8037e5e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
